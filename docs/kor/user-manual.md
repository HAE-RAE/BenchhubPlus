# 사용자 매뉴얼

BenchHub Plus를 활용해 LLM 평가를 수행하는 방법을 단계별로 안내합니다.

## 🎯 개요
자연어로 평가 요구사항을 입력하면 모델 설정, 벤치마크 선택, 결과 분석까지 한 번에 처리할 수 있습니다.

## 🚀 시작하기
1. 브라우저에서 애플리케이션 URL에 접속합니다.
   - 개발 환경: http://localhost:8501 (또는 8502)
   - 운영 환경: 배포된 도메인
2. 메인 화면은 다음 네 개의 탭으로 구성됩니다.
   - **Evaluate**: 평가 생성
   - **Status**: 진행 상황 모니터링
   - **Browse**: 과거 리더보드 조회
   - **System**: 시스템 상태 확인

## 📝 평가 생성
### 1단계: 자연어 질의 작성
- 예시: "한국 고등학교 수학 문제에서 GPT-4와 Claude-3을 비교해줘"
- 주제, 언어, 난이도, 과업 유형을 구체적으로 적을수록 정확한 계획이 생성됩니다.

### 2단계: 모델 설정
- 필수 항목: 모델 이름, API Base URL, API 키, 모델 유형(OpenAI, Anthropic 등)
- 고급 설정: Temperature, Max Tokens, Timeout
- 여러 모델을 비교하려면 모델 수를 늘리고 각각 설정합니다.

### 3단계: 평가 기준(선택)
- 언어, 주제(Subject Type), 과업(Task Type), 난이도, 샘플 수(10~1000), 지표(accuracy, f1 등)를 지정할 수 있습니다.

### 4단계: 평가 시작
1. 설정을 확인한 후 **"🚀 Start Evaluation"** 클릭
2. 반환된 Task ID를 확인하여 Status 탭에서 추적합니다.

## 📊 진행 상황 모니터링
- **Status** 탭에서 작업 목록과 현재 상태(PENDING/STARTED/SUCCESS/FAILURE)를 확인합니다.
- 작업 상세 정보에는 진행률, 시작/완료 시각, 오류 메시지가 표시됩니다.
- 실시간 갱신이 자동으로 이뤄지며 필요 시 "🔄 Refresh Status" 버튼을 이용하세요.

## 🏆 결과 해석
### 리더보드
- 모델별 점수, 정확도, 샘플 수, 실행 시간을 포함한 인터랙티브 테이블을 제공합니다.
- 차트로 상대 성능을 비교하고 정렬 기능으로 상위 모델을 빠르게 확인할 수 있습니다.

### 상세 결과
- "Detailed Results" 섹션에서 개별 샘플 결과, 오류 사례, 통계 분석을 열람합니다.
- CSV/JSON 등 내보내기 옵션을 통해 추가 분석이 가능합니다.

## 🔍 과거 데이터 탐색
- **Browse** 탭에서 언어, 주제, 과업, 기간, 점수 범위로 필터링합니다.
- 정렬/검색 기능을 통해 특정 모델이나 평가를 빠르게 찾을 수 있습니다.
- 시각화 도구로 모델별 추세 변화를 확인합니다.

## ⚙️ 시스템 관리
- **System** 탭에서 데이터베이스/Redis/API/워커 상태를 확인합니다.
- 큐 길이, 캐시 사용량, 리소스 통계 등 운영 지표를 제공합니다.
- 문제 발생 시 오류 메시지와 로그 경로가 표시됩니다.

## 🔧 고급 기능
- **질의 템플릿**: 자주 사용하는 평가 문구를 저장해 재사용합니다.
- **커스텀 데이터셋**: BenchHub 필터와 함께 자체 데이터셋을 연동합니다.
- **Webhook/REST 연동**: 외부 시스템에서 평가를 트리거하고 결과를 수집합니다.
- **Export**: 리더보드와 상세 결과를 파일로 내보낼 수 있습니다.

## 🙋 도움 받기
1. 시스템 탭에서 서비스 상태 확인
2. Status 탭에서 오류 메시지 확인
3. 모델 API 키 및 권한 점검
4. [트러블슈팅 가이드](troubleshooting.md) 참고
5. 해결되지 않을 경우 GitHub Issues 또는 운영 담당자에게 문의

## 참고 문서
- [빠른 시작](quickstart.md)
- [API 레퍼런스](api-reference.md)
- [BenchHub 구성 가이드](BENCHHUB_CONFIG.md)
